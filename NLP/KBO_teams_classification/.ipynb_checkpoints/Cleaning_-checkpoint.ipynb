{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b830803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# KBO 클래스 선언\n",
    "class KBOTeam:\n",
    "    def __init__(self):\n",
    "        self.win_lose = {}\n",
    "        self.corpus = {}\n",
    "    \n",
    "    def addWinLose(self, date, value):    # 승패 정보 딕셔너리에 데이터 추가\n",
    "        self.win_lose[date] = value\n",
    "    \n",
    "    def addCorpus(self, date, corpus):    # 코퍼스 데이터 딕셔너리에 데이터 추가\n",
    "        self.corpus[date] = corpus\n",
    "    \n",
    "    def __str__(self):    # 객체 자체를 출력했을 때, 출력 형식을 정함\n",
    "        return f'{self.win_lose}, {self.corpus}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905d11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 불러오기\n",
    "\n",
    "# 구단 이름 명 = 피클 파일명\n",
    "kbo_team_names = ['samsung', 'kia', 'lotte']\n",
    "kbo_teams = []    # 구단 객체를 담을 배열\n",
    "for name in kbo_team_names:\n",
    "    with open(f'../../datasets/kbo_corpus/{name}.pickle', 'rb') as fr:\n",
    "        kbo_teams.append(pickle.load(fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e4fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 입출력 함수 선언\n",
    "\n",
    "# 파일 Write\n",
    "def write_text(doc_arr, filename):    # win/lose_filename 형태\n",
    "    # string형태로 합치기\n",
    "    samsung_str = '\\n'.join(doc_arr[0])\n",
    "    kia_str = '\\n'.join(doc_arr[1])\n",
    "    lotte_str = '\\n'.join(doc_arr[2])\n",
    "    # 파일로 쓰기\n",
    "    with open(f\"../../datasets/kbo_corpus/samsung_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(samsung_str)\n",
    "    with open(f\"../../datasets/kbo_corpus/kia_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(kia_str)\n",
    "    with open(f\"../../datasets/kbo_corpus/lotte_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(lotte_str)\n",
    "\n",
    "# 파일 Read\n",
    "def read_text(filename):\n",
    "    results = []\n",
    "    with open(f\"../../datasets/kbo_corpus/samsung_{filename}.txt\", 'r') as fr:\n",
    "        samsung = fr.read()\n",
    "    results.append(samsung.split('\\n'))\n",
    "    with open(f\"../../datasets/kbo_corpus/kia_{filename}.txt\", 'r') as fr:\n",
    "        kia = fr.read()\n",
    "    results.append(kia.split('\\n'))\n",
    "    with open(f\"../../datasets/kbo_corpus/lotte_{filename}.txt\", 'r') as fr:\n",
    "        lotte = fr.read()\n",
    "    results.append(lotte.split('\\n'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4094637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 팀별 코퍼스 데이터만 분리하여 저장\n",
    "\n",
    "teams_corpus = []\n",
    "\n",
    "for team in kbo_teams:\n",
    "    corpus = []\n",
    "    for date in team.corpus:\n",
    "        corpus += team.corpus[date]\n",
    "    teams_corpus.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb1c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20302\n",
      "13850\n",
      "22421\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(len(teams_corpus[i]))\n",
    "write_text(teams_corpus, 'document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07aa95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 시작\n",
    "# <1> Basic Processing\n",
    "# 1. 문장 부호 대체\n",
    "# 2. 문장 토큰화는 생략\n",
    "def clean_punc(text):\n",
    "    punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "    \n",
    "    for p in punct_mapping:\n",
    "        text = text.replace(p, punct_mapping[p])\n",
    "    \n",
    "    return text.strip()    # 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51fdc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정해영 올림픽 간다고?한심 사토한테 털릴준비해라',\n",
       " '작년에 황대인 기회 안준거 진짜 아깝다',\n",
       " '작년 머인이 1군무대 처음으로 100타석이상 한건데',\n",
       " '이민우 17데뷔승 이후 첫승이냐',\n",
       " '야구장 갈라면 케텍스에서 한참 가야되잖아 니네']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = read_text('document')\n",
    "\n",
    "punc1_cleaned_documents = [[], [], []]\n",
    "\n",
    "for i in range(3):\n",
    "    for sen in documents[i]:\n",
    "        punc1_cleaned_documents[i].append(clean_punc(sen))\n",
    "\n",
    "punc1_cleaned_documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01e212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 문장 부호, 숫자, html 태그, 공백 제거 + 대문자 소문자로\n",
    "import re\n",
    "\n",
    "def clean_corpus(origin_texts):\n",
    "    cleaning_texts = []\n",
    "    for sen in origin_texts:    # r'문자열' : raw 문자열. \\기호 그대로 출력\n",
    "        cleaning_sen = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '', sen)    #remove punctuation\n",
    "        # cleaning_sen = re.sub(r'\\d+', '', cleaning_sen)    # remove number\n",
    "        cleaning_sen = cleaning_sen.lower()    # lower case\n",
    "        cleaning_sen = re.sub(r'\\s+', ' ', cleaning_sen)    # remove extra space\n",
    "        cleaning_sen = re.sub(r'<[^>]+>', '', cleaning_sen)    # remove spaces\n",
    "        cleaning_sen = re.sub(r'^\\s+', '', cleaning_sen)    # remove space from start\n",
    "        cleaning_sen = re.sub(r'\\s+$', '', cleaning_sen)    # remove space from the end\n",
    "        cleaning_texts.append(cleaning_sen)\n",
    "    return cleaning_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec130a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정해영 올림픽 간다고한심 사토한테 털릴준비해라',\n",
       " '작년에 황대인 기회 안준거 진짜 아깝다',\n",
       " '작년 머인이 1군무대 처음으로 100타석이상 한건데',\n",
       " '이민우 17데뷔승 이후 첫승이냐',\n",
       " '야구장 갈라면 케텍스에서 한참 가야되잖아 니네']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc_cleaned_documents = []\n",
    "\n",
    "for i in range(3):\n",
    "    punc_cleaned_documents.append(clean_corpus(punc1_cleaned_documents[i]))\n",
    "\n",
    "punc_cleaned_documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb1d98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <2> Spell check --> 생략\n",
    "# <3> 단어 토큰화 + Stemming\n",
    "\n",
    "# 1. 단어 토큰화 : khaiii 라이브러리 사용\n",
    "from khaiii import KhaiiiApi\n",
    "\n",
    "# 명사, 형용사, 동사, 부사만 남겨 둠\n",
    "def pos_corpus(document):\n",
    "    api = KhaiiiApi()\n",
    "    significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "    \n",
    "    corpus = []\n",
    "    for sen in document:\n",
    "        if sen == '':    # 예외처리\n",
    "            continue\n",
    "        pos_tagged = ''\n",
    "        for word in api.analyze(sen):\n",
    "            for morph in word.morphs:\n",
    "                if morph.tag in significant_tags:\n",
    "                    pos_tagged += str(morph) + ' '\n",
    "        corpus.append(pos_tagged.strip())\n",
    "        # 비교\n",
    "        # print(sen)\n",
    "        # print(pos_tagged.strip())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0525098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정해영/NNP 올림픽/NNG 가/VV 한심/NNG 사토/NNG 털릴준비/NNG 하/XSV',\n",
       " '작년/NNG 황대/NNG 기회/NNG 안준거/NNP 진짜/MAG 아깝/VA',\n",
       " '작년/NNG 머인/NNG 군무대/NNG 처음/NNG 타석이상/NNG 한/NNG 것/NNB',\n",
       " '이민우/NNP 데뷔승/NNG 이후/NNG 승/NNG',\n",
       " '야구장/NNG 가/VV 라/VV 케텍스/NNP 한참/NNG 가/VV 야/NNG 되/XSV 않/VX']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_documents = []\n",
    "\n",
    "for i in range(3):\n",
    "    pos_tagged_documents.append(pos_corpus(punc_cleaned_documents[i]))\n",
    "pos_tagged_documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f262e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. stemming 수행\n",
    "def stemming_corpus(document):\n",
    "    p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "    p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "    p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "    p4 = re.compile('[가-힣A-Za-z0-9]+/VX')\n",
    "    \n",
    "    corpus = []\n",
    "    for sen in document:\n",
    "        ori_sen = sen\n",
    "        \n",
    "        # 1번 경우\n",
    "        mached_terms = re.findall(p1, ori_sen)  \n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sen = ori_sen.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        # 2번 경우\n",
    "        mached_terms = re.findall(p2, ori_sen)  \n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sen = ori_sen.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        # 3번 경우\n",
    "        mached_terms = re.findall(p3, ori_sen)  \n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if modi_terms[-1] != '다':\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sen = ori_sen.replace(ori_terms, modi_terms)\n",
    "            \n",
    "        # 4번 경우\n",
    "        mached_terms = re.findall(p4, ori_sen)  \n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if modi_terms[-1] != '다':\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sen = ori_sen.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        corpus.append(ori_sen)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7777f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정해영/NNP 올림픽/NNG 가다/VV 한심/NNG 사토/NNG 털릴준비하다/VV',\n",
       " '작년/NNG 황대/NNG 기회/NNG 안준거/NNP 진짜/MAG 아깝/VA',\n",
       " '작년/NNG 머인/NNG 군무대/NNG 처음/NNG 타석이상/NNG 한/NNG 것/NNB',\n",
       " '이민우/NNP 데뷔승/NNG 이후/NNG 승/NNG',\n",
       " '야구장/NNG 가다/VV 라다/VV 케텍스/NNP 한참/NNG 가다/VV 야되다/VV 않다/VV']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_documents = []\n",
    "for i in range(3):\n",
    "    stemming_documents.append(stemming_corpus(pos_tagged_documents[i]))\n",
    "stemming_documents[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f789ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text(stemming_documents, 'cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
