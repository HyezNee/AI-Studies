{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5bd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d68856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 입출력 함수 선언\n",
    "\n",
    "# 파일 Write\n",
    "def write_text(doc_arr, filename):    # win/lose_filename 형태\n",
    "    # string형태로 합치기\n",
    "    samsung_str = '\\n'.join(doc_arr[0])\n",
    "    kia_str = '\\n'.join(doc_arr[1])\n",
    "    lotte_str = '\\n'.join(doc_arr[2])\n",
    "    # 파일로 쓰기\n",
    "    with open(f\"../../datasets/kbo_corpus/samsung_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(samsung_str)\n",
    "    with open(f\"../../datasets/kbo_corpus/kia_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(kia_str)\n",
    "    with open(f\"../../datasets/kbo_corpus/lotte_{filename}.txt\", 'w') as fw:\n",
    "        fw.write(lotte_str)\n",
    "\n",
    "# 파일 Read\n",
    "def read_text(filename):\n",
    "    results = []\n",
    "    with open(f\"../../datasets/kbo_corpus/samsung_{filename}.txt\", 'r') as fr:\n",
    "        samsung = fr.read()\n",
    "    results.append(samsung.split('\\n'))\n",
    "    with open(f\"../../datasets/kbo_corpus/kia_{filename}.txt\", 'r') as fr:\n",
    "        kia = fr.read()\n",
    "    results.append(kia.split('\\n'))\n",
    "    with open(f\"../../datasets/kbo_corpus/lotte_{filename}.txt\", 'r') as fr:\n",
    "        lotte = fr.read()\n",
    "    results.append(lotte.split('\\n'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c57c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = read_text('cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411d8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정해영/NNP 올림픽/NNG 가다/VV 한심/NNG 사토/NNG 털릴준비하다/VV',\n",
       " '작년/NNG 황대/NNG 기회/NNG 안준거/NNP 진짜/MAG 아깝/VA',\n",
       " '작년/NNG 머인/NNG 군무대/NNG 처음/NNG 타석이상/NNG 한/NNG 것/NNB',\n",
       " '이민우/NNP 데뷔승/NNG 이후/NNG 승/NNG',\n",
       " '야구장/NNG 가다/VV 라다/VV 케텍스/NNP 한참/NNG 가다/VV 야되다/VV 않다/VV',\n",
       " '소형준/NNG 신인/NNG 시즌/NNG 경기/NNG 방어/NNG 점/NNB 대/NNG',\n",
       " '올해/NNG 쓰다/VV 않다/VV 거/NNB 씹민상/NNG 왜/MAG 썻/NNG',\n",
       " '근데/MAJ 설다/VV 프로필/NNG 존나/NNG 사/NNG 잘/MAG 찍다/VV 주다/VV',\n",
       " '황대/NNG 페/NNG 이/NNP 스북/NNG',\n",
       " '그러다/VV 황대인/NNG 점점/MAG 공/NNG 맞히다/VV']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_documents[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eec4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <1> 라벨을 생성해서 X, y에 넣기\n",
    "# 이 때 y값은 라벨 인코딩을 해 준다. (0 : 삼성, 1 : 기아, 2 : 롯데)\n",
    "def load_data(cleaned_documents):\n",
    "    X, y = [], []\n",
    "    for i in range(3):\n",
    "        for sen in cleaned_documents[i]:\n",
    "            sentence = []\n",
    "            for word in sen.split():\n",
    "                lemma = word.split('/')[0] + ' '\n",
    "                sentence.append(lemma.strip())\n",
    "            X.append(sentence)\n",
    "            y.append(i)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37555642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28281"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ori, y_ori = load_data(cleaned_documents)\n",
    "X, y = [], []\n",
    "for i, sen in enumerate(X_ori):\n",
    "    if i % 2 == 0:\n",
    "        X.append(sen)\n",
    "        y.append(y_ori[i])\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42ad3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 데이터 수집\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cead15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM 만들기\n",
    "def make_DTM(X_train, X_test):\n",
    "    # 사전 작성\n",
    "    dictionary = gensim.corpora.Dictionary(X_train)\n",
    "\n",
    "    #train\n",
    "    corpus = [dictionary.doc2bow(sen) for sen in X_train]\n",
    "    tfidf_model = models.TfidfModel(dictionary=dictionary)\n",
    "    tfidf_corpus = tfidf_model[corpus]\n",
    "    num_words = len(dictionary)\n",
    "    num_sen = len(X_train)\n",
    "    X_train_dtm = gensim.matutils.corpus2dense(tfidf_corpus, num_terms=num_words, num_docs=num_sen).T\n",
    "\n",
    "    # test\n",
    "    corpus_test = [dictionary.doc2bow(sen) for sen in X_test]\n",
    "    tfidf_corpus_test = tfidf_model[corpus_test]\n",
    "    num_sen_test = len(X_test)\n",
    "    X_test_dtm = gensim.matutils.corpus2dense(tfidf_corpus_test, num_terms=num_words, num_docs=num_sen_test).T\n",
    "    \n",
    "    return X_train_dtm, X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21416bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm, X_test_dtm = make_DTM(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9190cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(X_train_dtm, X_test_dtm, y_train, y_test):\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca30e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.66      2521\n",
      "           1       0.72      0.48      0.58      1760\n",
      "           2       0.64      0.76      0.70      2790\n",
      "\n",
      "    accuracy                           0.66      7071\n",
      "   macro avg       0.67      0.64      0.64      7071\n",
      "weighted avg       0.66      0.66      0.65      7071\n",
      "\n",
      "[[1674  168  679]\n",
      " [ 399  851  510]\n",
      " [ 503  161 2126]]\n"
     ]
    }
   ],
   "source": [
    "clf, y_pred = fit_and_predict(X_train_dtm, X_test_dtm, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae564e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : ['균안', '키', '박다', 'ㄹㅇ', '보다', '싶다']\n",
      "real_y : 롯데\n",
      "pred_y : 롯데\n",
      "X : ['욕', '먹다', '싫', '마무리', '스트', '꽂다', 'ㅋㅋㅋ']\n",
      "real_y : 롯데\n",
      "pred_y : 롯데\n",
      "X : ['근데', '진짜', '허윤동', '평속', '따리', '기대', '안', '됨']\n",
      "real_y : 삼성\n",
      "pred_y : 삼성\n",
      "X : ['강민호', '박해민', '백정현', '금액', '얼마', '예상하다']\n",
      "real_y : 삼성\n",
      "pred_y : 삼성\n",
      "X : ['드', '랲', '진', '짜', '야', '잘잘', '드랲이네']\n",
      "real_y : 롯데\n",
      "pred_y : 삼성\n",
      "X : ['추재', '현', '새끼', '넘다']\n",
      "real_y : 롯데\n",
      "pred_y : 롯데\n",
      "X : ['안치홍', '팔수', '있다']\n",
      "real_y : 롯데\n",
      "pred_y : 롯데\n",
      "X : ['세웅', '이', '승리못', '주다', '팀', '너무', '원망스럽다']\n",
      "real_y : 롯데\n",
      "pred_y : 롯데\n",
      "X : ['이의리', '회', '올리다', '하다']\n",
      "real_y : KIA\n",
      "pred_y : KIA\n",
      "X : ['며칠전', '갸']\n",
      "real_y : 롯데\n",
      "pred_y : KIA\n"
     ]
    }
   ],
   "source": [
    "id2label = {0 : '삼성', 1 : 'KIA', 2 : '롯데'}\n",
    "for i in range(6984, 6994):\n",
    "    print(f'X : {X_test[i]}')\n",
    "    print(f'real_y : {id2label[y_test[i]]}')\n",
    "    print(f'pred_y : {id2label[y_pred[i]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
